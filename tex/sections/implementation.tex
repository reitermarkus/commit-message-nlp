\section{Implementation}
\label{sec:implementation}

The implementation itself is comprised of three parts:

\begin{itemize}
  \item Scraping
  \item Tagging
  \item Training/Testing
\end{itemize}

\subsection{Scraping}

The scraping process is a curious case since it was problematic to some extent.
More in that regard can be found in \autoref{sec:challenges}. The basic idea
of the scraping process was to fetch repositories and their commits from
GitHub. This was done with the help of PyGithub. The code itself for the
process of repository gathering is rather simple. A basic array with all
languages was defined as seen in \autoref{lst:languages}, which is then used to
iterate over and actually search for repositories for the corresponding language
with the specific GitHub query syntax as seen in \autoref{lst:query}.

\begin{lstlisting}[language=python, label={lst:languages}, caption={Array of all languages for scraping}]
languages = [
  'python',
  'javascript',
  'go',
  'typescript',
  'rust',
  'kotlin',
  'java',
  'c++',
  'c#',
  'swift',
]
\end{lstlisting}

\begin{lstlisting}[language=python, label={lst:query}, caption={GitHub query syntax}]
g.search_repositories(
  query=f'language:{language}', sort='stars', order='desc'
)
\end{lstlisting}

This then yields all repositories sorted by stars in descending order.
Stars is a metric on GitHub to show a repository's popularity. Each star
represents a user on GitHub who has starred this repository. Counters keep track of
per-author (100), per-repository (10000) and global (1000000) commit limits.
This means that at least 10 repositories are per per language. When the
\whitelist{100000th} commit is reached, repositories for the next language will
be fetched. Because the process of scraping is time and space intensive, the
results are cached. Per language, one CSV file is created. The file consists of
rows with four columns:

\begin{itemize}
  \item repository
  \item language
  \item author
  \item message
\end{itemize}

With this structure, all important information is included in the file which is
essential for further processing. An example excerpt of one file can be seen in
\autoref{lst:csv}.

\begin{lstlisting}[language=xml, label={lst:csv}, caption={Excerpt of \texttt{typescript.csv} with commits from the repository \texttt{microsoft/vscode}}]
repository,language,author,message
microsoft/vscode,typescript,roblourens@gmail.com,"Fix #126087"
microsoft/vscode,typescript,penn.lv@gmail.com,":notebook: differenciate editor focus and list view focus"
microsoft/vscode,typescript,me@tylerleonhardt.com,"always hide quickinput on iPad when focus is lost fixes #125284"
microsoft/vscode,typescript,connor@peet.io,"fix: use inline sourcemaps in watch task"
microsoft/vscode,typescript,alexdima@microsoft.com,"update `monaco.d.ts`"
\end{lstlisting}

\subsection{Tagging}

The next step is to tag commits. We distinguish between two categories of tagged
commits:

\begin{itemize}
  \item Conventional Commits \cite{conventionalcommits}
  \item Gitmoji \cite{gitmoji}
\end{itemize}

A specific regular expression for each category has to be matched in order to
tag the commit. The lookup in case of a match is done by searching for the
specific tag in a dictionary. Gitmoji commits are mapped to follow the same naming
as Conventional Commits using the lookup table seen in \autoref{lst:gitmoji_map}.
Commits which follow the Conventional Commits format are mapped using \autoref{lst:tag_map}.
In the end, all commits which are tagged following the Angular Commit Message
Guidelines \cite{angular_guidelines} are treated as “known tags” and filtered.
In addition to adding a tag to a commit, the tag is also removed from the original
commit message.

\begin{lstlisting}[language=python, label={lst:gitmoji_map}, caption={Dictionary for Gitmoji mappings}]
gitmoji_mappings = {
  ':memo:':     'docs',     # Documentation
  ':zap:':      'perf',     # Performance
  ':fire:':     'remove',   # Removal
  ':sparkles:': 'feat',     # Feature
  ':bug:':      'fix',      # Bug Fix
  ':recycle:':  'refactor', # Refactor Code
  ...
}
\end{lstlisting}

\begin{lstlisting}[language=python, label={lst:tag_map}, caption={Dictionary for conventional tag mappings}]
tag_mappings = {
  'bug':           'fix',
  'testing':       'test',
  'documentation': 'docs',
  'feature':       'feat',
  'gui':           'ui',
  ...
}
\end{lstlisting}

\subsection{Training \& Testing}

In order to properly train and test the dataset, some filtering has to be done.
With distinct regular expressions common message patterns are filtered from
the commit message. With \lstinline{#\d+}, pull request or issue numbers
are normalised, since they start with a \# and end with a number. Version
numbers can also exacerbate tokenisation and therefore the simple regular
expression \lstinline{\b\d+(\.\d+)+\b} takes care of basic version notations
that are common on GitHub. Email addresses and URLs in general also make
tokenisation more difficult. Therefore, both of those patterns are also
removed.

The next step is then to encode the tags. This can be done with the
\textit{LabelEncoder} from \textit{sklearn}. The \textit{sklearn} library also
offers a \textit{CountVectorizer} with which text documents can be converted
into token counts. The actual training/testing process is done using a
10-fold cross validation which splits the data and yields train and test
indices with which the train and test data can be defined. Both datasets are
then used within the logistic regression to get the final results.
